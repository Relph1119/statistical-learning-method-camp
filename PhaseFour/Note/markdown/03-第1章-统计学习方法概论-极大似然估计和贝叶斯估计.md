## 第1章-统计学习方法概论-极大似然估计和贝叶斯估计

### 极大似然估计
&emsp;&emsp;在掷硬币实验中估计出现正面向上的概率$\theta$，通过一系列的实验，就会得到很多个观测的结果，将每个观测的结果用随机变量表示出来$x_i = \left\{ \begin{array}{ll}
1 & \textrm{正}\\
0 & \textrm{反}
\end{array} \right.$，$x_i$满足二项分布$x_i \sim B(1, \theta)$，概率函数为$P(X=x)=\theta^x(1-\theta)^{1-x}$。  
&emsp;&emsp;假设已知$\theta$，根据实验结果，得到出现这组结果的概率用$L$表示，其似然函数为$\displaystyle L(\theta)=\prod_{i=1}^n P(X_i=x_i|\theta)$（为什么可以写成连乘形式？因为每一次投硬币的概率都是独立的）。将概率函数带入到似然函数中，得到$\displaystyle L(\theta)=\prod_{i=1}^n \theta^{x_i} (1-\theta)^{1-x_i}$。  
&emsp;&emsp;极大似然估计的意义：找到一个$\theta$使得样本出现的概率是最大的，也就是需要最大化似然函数。最大化似然函数等价于最大化这个似然函数的对数：$$\begin{aligned}
\max \ln L(\theta)
& = \sum [\ln \theta^{x_i} + \ln (1- \theta)^{1-x_i} ] \\
& = \sum x_{i} \ln \theta + (n-\sum x_i) \ln (1-\theta) \\
\end{aligned}$$
对$\ln L(\theta)$求导：$\frac{\partial \ln L(\theta)}{\partial \theta} = \frac{\sum x_{i}}{\theta}-\frac{n - \sum x_{i}}{1-\theta}=0 $，得到$\hat\theta= \frac{\sum x_i}{n}$。  
&emsp;&emsp;概括来说，极大似然估计就是根据样本的概率分布，写出样本的联合概率的似然函数，然后通过最大化似然函数，得到参数的估计值。  
&emsp;&emsp;极大似然估计完全是根据样本信息得到的参数估计，其参数估计值为$\hat\theta= \frac{\sum x_i}{n}$。  

### 贝叶斯估计
&emsp;&emsp;先验概率密度函数：$\pi(\theta)=\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}$，在$[0,1]$区间中，$\theta$的概率密度函数由$\alpha和\beta$来决定的。这里不给$\alpha$和$\beta$赋值，在实际应用过程中，需要进行赋值的。  
&emsp;&emsp;目前已知$\pi(\theta)$和一组样本$x_1,x_2,\dots,x_n$，根据样本信息调整对$\theta$分布的判断，即找到对应的后验分布：$$\begin{aligned} p(\theta|x_i,\dots,x_n)
&= \frac{p\left(\theta, x_{1} \cdots, x_{n}\right)}{p\left(x_{1}, \dots, x_{n}\right)} \\
&= \frac{\pi(\theta) \cdot p(x_{1} | \theta) \cdots p(x_{n} | \theta)}{\int p(\theta, x_{1}, \cdots, x_{n}) d \theta} \\
& \propto \theta^{\alpha-1}(1 - \theta)^{\beta - 1} \prod \theta^{x_i}(1-\theta)^{1-x_i} \\
&= \theta^{\sum x_i+\alpha-1}(1-\theta)^{n-\sum x_i+\beta-1} 
\end{aligned}$$
&emsp;&emsp;上式得到的最终结果也是一个贝叶斯分布，参数为$\sum x_i+\alpha$和$n-\sum x_i+\beta$，所以在贝叶斯估计中，首先得到的是一个关于参数给定样本信息的后验分布，然后要给出$\theta$一个具体的值来估计它，就从这个后验分布中，找出使得后验分布的概率密度最大（即众数），可得到众数值为$\hat{\theta}=\frac{\sum x_i+\alpha-1}{n+\alpha+\beta-2}$。

### 对比极大似然估计和贝叶斯估计
极大似然估计（MLE）：$\hat\theta= \frac{\sum x_i}{n}$  
贝叶斯估计（Bayes）：$\hat{\theta}=\frac{\sum x_i+\alpha-1}{n+\alpha+\beta-2}$  
&emsp;&emsp;当样本量$n \rightarrow \infty$时，贝叶斯估计的结果为$\frac{\sum x_i}{n}$，可以看到这个结果就是极大似然估计的结果。  
**解释：**   
&emsp;&emsp;首先会在贝叶斯估计中，给出参数的先验信息，但是当样本量足够大的时候，之前的先验信息与样本信息相比，就非常的微不足道了。所以近似于用所有的样本信息估计$\theta$得到的结果。那为什么还会有贝叶斯估计呢？由于之前考虑的样本量是很大的情况，现在考虑一个极端情况，假如只有一个样本，通过极大似然估计，$\theta$只能为0或1，但在贝叶斯估计中，如果得到的样本取值为0，得到的结果是$\frac{\alpha-1}{\alpha+\beta-1}$，如果得到的样本取值为1，得到的结果是$\frac{\alpha}{\alpha+\beta-1}$。  
&emsp;&emsp;对比可得，由贝叶斯估计得到的结果不会像极大似然估计那么极端，所以当样本量小的时候，这个就是贝叶斯估计的优势所在，当样本量大的时候，这两个估计的结果是一样的。